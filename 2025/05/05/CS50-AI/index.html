<!DOCTYPE html>


<html theme="dark" showBanner="true" hasBanner="desktop" > 
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet">
<link href="https://cdn.staticfile.org/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet">
<script src="/js/color.global.min.js" ></script>
<script src="/js/load-settings.js" ></script>
<head>
  <meta charset="utf-8">
  
  
  
<!-- Gaug.es Analytics -->
<script>
  var _gauges = _gauges || [];
  (function() {
    var t   = document.createElement('script');
    t.async = true;
    t.id    = 'gauges-tracker';
    t.setAttribute('data-site-id', 'true');
    t.setAttribute('data-track-path', 'https://track.gaug.es/track.gif');
    t.src = 'https://d36ee2fcip1434.cloudfront.net/track.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(t, s);
  })();
</script>
<!-- End Gaug.es Analytics -->


  
  <title>CS50-AI | Smiling</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="preload" href="/css/fonts/Roboto-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
  <link rel="preload" href="/css/fonts/Roboto-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">

  <meta name="description" content="This blog is used to note down all the knowledge I’ve learned in this lesson.  Here is some links to this lesson. Home Page and Gradebook About submitting projects: check50 , submit50 , my submit , P">
<meta property="og:type" content="article">
<meta property="og:title" content="CS50-AI">
<meta property="og:url" content="http://example.com/2025/05/05/CS50-AI/index.html">
<meta property="og:site_name" content="Smiling">
<meta property="og:description" content="This blog is used to note down all the knowledge I’ve learned in this lesson.  Here is some links to this lesson. Home Page and Gradebook About submitting projects: check50 , submit50 , my submit , P">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2025/05/05/CS50-AI/images/cs50ai.png">
<meta property="article:published_time" content="2025-05-05T08:44:36.000Z">
<meta property="article:modified_time" content="2025-05-20T13:01:12.283Z">
<meta property="article:author" content="Smiling">
<meta property="article:tag" content="csdiy">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/05/05/CS50-AI/images/cs50ai.png">
  
    <link rel="alternate" href="/atom.xml" title="Smiling" type="application/atom+xml">
  
  
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: light)" href="/images/favicon-light-192.png" sizes="192x192">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-32.png" sizes="32x32">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-128.png" sizes="128x128">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-180.png" sizes="180x180">
    <link rel="icon" media="(prefers-color-scheme: dark)" href="/images/favicon-dark-192.png" sizes="192x192">
  
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

<body>
  
  
    
<div id="banner" class="">
  <img src="/images/banner1.png" itemprop="image">
  <div id="banner-dim"></div>
</div>
 
   
  <div id="main-grid" class="  ">
    <div id="nav" class=""  >
      <navbar id="navbar">
  <nav id="title-nav">
    <a href="/">
      <div id="vivia-logo">
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
        <div class="dot"></div>
      </div>
      <div>Smiling </div>
    </a>
  </nav>
  <nav id="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    <a id="theme-btn" class="nav-icon">
      <span class="light-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M438.5-829.913v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-829.913Zm0 747.826v-48q0-17.452 11.963-29.476 11.964-12.024 29.326-12.024 17.363 0 29.537 12.024t12.174 29.476v48q0 17.452-11.963 29.476-11.964 12.024-29.326 12.024-17.363 0-29.537-12.024T438.5-82.087ZM877.913-438.5h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537t29.476-12.174h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T877.913-438.5Zm-747.826 0h-48q-17.452 0-29.476-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T82.087-521.5h48q17.452 0 29.476 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T130.087-438.5Zm660.174-290.87-34.239 32q-12.913 12.674-29.565 12.174-16.653-.5-29.327-13.174-12.674-12.673-12.554-28.826.12-16.152 12.794-28.826l33-35q12.913-12.674 30.454-12.674t30.163 12.847q12.709 12.846 12.328 30.826-.38 17.98-13.054 30.653ZM262.63-203.978l-32 34q-12.913 12.674-30.454 12.674t-30.163-12.847q-12.709-12.846-12.328-30.826.38-17.98 13.054-30.653l33.239-31q12.913-12.674 29.565-12.174 16.653.5 29.327 13.174 12.674 12.673 12.554 28.826-.12 16.152-12.794 28.826Zm466.74 33.239-32-33.239q-12.674-12.913-12.174-29.565.5-16.653 13.174-29.327 12.673-12.674 28.826-13.054 16.152-.38 28.826 12.294l35 33q12.674 12.913 12.674 30.454t-12.847 30.163q-12.846 12.709-30.826 12.328-17.98-.38-30.653-13.054ZM203.978-697.37l-34-33q-12.674-12.913-13.174-29.945-.5-17.033 12.174-29.707t31.326-13.293q18.653-.62 31.326 13.054l32 34.239q11.674 12.913 11.174 29.565-.5 16.653-13.174 29.327-12.673 12.674-28.826 12.554-16.152-.12-28.826-12.794ZM480-240q-100 0-170-70t-70-170q0-100 70-170t170-70q100 0 170 70t70 170q0 100-70 170t-170 70Zm-.247-82q65.703 0 111.475-46.272Q637-414.544 637-480.247t-45.525-111.228Q545.95-637 480.247-637t-111.475 45.525Q323-545.95 323-480.247t45.525 111.975Q414.05-322 479.753-322ZM481-481Z"/></svg></span>
      <span class="dark-mode-icon"><svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M480.239-116.413q-152.63 0-258.228-105.478Q116.413-327.37 116.413-480q0-130.935 77.739-227.435t206.304-125.043q43.022-9.631 63.87 10.869t3.478 62.805q-8.891 22.043-14.315 44.463-5.424 22.42-5.424 46.689 0 91.694 64.326 155.879 64.325 64.186 156.218 64.186 24.369 0 46.978-4.946 22.609-4.945 44.413-14.076 42.826-17.369 62.967 1.142 20.142 18.511 10.511 61.054Q807.174-280 712.63-198.206q-94.543 81.793-232.391 81.793Zm0-95q79.783 0 143.337-40.217 63.554-40.218 95.793-108.283-15.608 4.044-31.097 5.326-15.49 1.283-31.859.805-123.706-4.066-210.777-90.539-87.071-86.473-91.614-212.092-.24-16.369.923-31.978 1.164-15.609 5.446-30.978-67.826 32.478-108.282 96.152Q211.652-559.543 211.652-480q0 111.929 78.329 190.258 78.329 78.329 190.258 78.329ZM466.13-465.891Z"/></svg></span>
    </a>
    
      <a id="nav-rss-link" class="nav-icon mobile-hide" href="/atom.xml" title="RSS Feed">
        <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M198-120q-25.846 0-44.23-18.384-18.384-18.385-18.384-44.23 0-25.846 18.384-44.23 18.384-18.385 44.23-18.385 25.846 0 44.23 18.385 18.384 18.384 18.384 44.23 0 25.845-18.384 44.23Q223.846-120 198-120Zm538.385 0q-18.846 0-32.923-13.769-14.076-13.769-15.922-33.23-8.692-100.616-51.077-188.654-42.385-88.039-109.885-155.539-67.5-67.501-155.539-109.885Q283-663.462 182.385-672.154q-19.461-1.846-33.23-16.23-13.769-14.385-13.769-33.846t14.076-32.922q14.077-13.461 32.923-12.23 120.076 8.692 226.038 58.768 105.961 50.077 185.73 129.846 79.769 79.769 129.846 185.731 50.077 105.961 58.769 226.038 1.231 18.846-12.538 32.922Q756.461-120 736.385-120Zm-252 0q-18.231 0-32.423-13.461t-18.653-33.538Q418.155-264.23 348.886-333.5q-69.27-69.27-166.501-84.423-20.077-4.462-33.538-18.961-13.461-14.5-13.461-33.346 0-19.076 13.884-33.23 13.884-14.153 33.115-10.922 136.769 15.384 234.384 112.999 97.615 97.615 112.999 234.384 3.231 19.23-10.538 33.115Q505.461-120 484.385-120Z"/></svg>
      </a>
    
    <div id="nav-menu-btn" class="nav-icon">
      <svg xmlns="http://www.w3.org/2000/svg" height="20" viewBox="0 -960 960 960" width="20"><path d="M177.37-252.282q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Zm0-186.218q-17.453 0-29.477-11.963-12.024-11.964-12.024-29.326 0-17.363 12.024-29.537T177.37-521.5h605.26q17.453 0 29.477 11.963 12.024 11.964 12.024 29.326 0 17.363-12.024 29.537T782.63-438.5H177.37Zm0-186.217q-17.453 0-29.477-11.964-12.024-11.963-12.024-29.326t12.024-29.537q12.024-12.174 29.477-12.174h605.26q17.453 0 29.477 11.964 12.024 11.963 12.024 29.326t-12.024 29.537q-12.024 12.174-29.477 12.174H177.37Z"/></svg>
    </div>
  </nav>
</navbar>
<div id="nav-dropdown" class="hidden">
  <div id="dropdown-link-list">
    
      <a class="nav-dropdown-link" href="/">Home</a>
    
      <a class="nav-dropdown-link" href="/archives">Archives</a>
    
      <a class="nav-dropdown-link" href="/about">About</a>
    
    
      <a class="nav-dropdown-link" href="/atom.xml" title="RSS Feed">RSS</a>
     
    </div>
</div>
<script>
  let dropdownBtn = document.getElementById("nav-menu-btn");
  let dropdownEle = document.getElementById("nav-dropdown");
  dropdownBtn.onclick = function() {
    dropdownEle.classList.toggle("hidden");
  }
</script>
    </div>
    <div id="sidebar-wrapper">
      <sidebar id="sidebar">
  
    <div class="widget-wrap">
  <div class="info-card">
    <div class="avatar">
      
        <image src=/images/avatar1.jpg></image>
      
      <div class="img-dim"></div>
    </div>
    <div class="info">
      <div class="username">Smiling </div>
      <div class="dot"></div>
      <div class="subtitle">ε = = (づ′▽`)づ </div>
      <div class="link-list">
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://twitter.com" title="Twitter"><i class="fa-brands fa-twitter"></i></a>
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://bilibili.com" title="Bilibili"><i class="fa-brands fa-bilibili"></i></a>
        
          <a class="link-btn" target="_blank" rel="noopener" href="https://github.com/smiling689" title="GitHub"><i class="fa-brands fa-github"></i></a>
         
      </div>  
    </div>
  </div>
</div>

  
  <div class="sticky">
    
      


  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Categories</h3>
      <div class="category-box">
            <a class="category-link" href="/categories/%E5%AD%A6%E4%B9%A0/">
                学习
                <div class="category-count">17</div>
            </a>
        
            <a class="category-link" href="/categories/%E6%9D%82%E8%B0%88/">
                杂谈
                <div class="category-count">2</div>
            </a>
        
            <a class="category-link" href="/categories/%E7%94%9F%E6%B4%BB/">
                生活
                <div class="category-count">1</div>
            </a>
        </div>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Tags</h3>
      <ul class="widget-tag-list" itemprop="keywords"><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/csdiy/" rel="tag">csdiy</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%A8%B1%E4%B9%90/" rel="tag">娱乐</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag">学习</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%AD%A6%E6%9C%AF%E5%86%99%E4%BD%9C/" rel="tag">学术写作</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E5%BA%9F%E8%AF%9D/" rel="tag">废话</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90/" rel="tag">数学分析</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%95%B0%E7%90%86%E9%80%BB%E8%BE%91/" rel="tag">数理逻辑</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E6%9D%82%E9%A1%B9/" rel="tag">杂项</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%89%A9%E7%90%86/" rel="tag">物理</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%94%9F%E6%B4%BB/" rel="tag">生活</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">程序设计与数据结构</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="widget-tag-list-item"><a class="widget-tag-list-link" href="/tags/%E7%BD%91%E7%AB%99%E5%BB%BA%E8%AE%BE/" rel="tag">网站建设</a></li></ul>
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Archives</h3>
      
      
        <a class="archive-link" href="/archives/2025/05 ">
          May 2025 
          <div class="archive-count">6 </div>
        </a>
      
        <a class="archive-link" href="/archives/2025/04 ">
          April 2025 
          <div class="archive-count">14 </div>
        </a>
      
    </div>
  </div>


    
      
  <div class="widget-wrap">
    <div class="widget">
      <h3 class="widget-title">Recent Posts</h3>
      <ul>
        
          <a class="recent-link" href="/2025/05/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%80%BB%E7%BB%93/" title="数据结构总结" >
            <div class="recent-link-text">
              数据结构总结
            </div>
          </a>
        
          <a class="recent-link" href="/2025/05/17/La-Vie-Boheme/" title="La Vie Bohème" >
            <div class="recent-link-text">
              La Vie Bohème
            </div>
          </a>
        
          <a class="recent-link" href="/2025/05/15/%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/" title="算法笔记" >
            <div class="recent-link-text">
              算法笔记
            </div>
          </a>
        
          <a class="recent-link" href="/2025/05/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%A4%87%E8%80%83%E7%AC%94%E8%AE%B0/" title="数据结构备考笔记" >
            <div class="recent-link-text">
              数据结构备考笔记
            </div>
          </a>
        
          <a class="recent-link" href="/2025/05/06/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" title="环境配置" >
            <div class="recent-link-text">
              环境配置
            </div>
          </a>
        
      </ul>
    </div>
  </div>

    
  </div>
</sidebar>
    </div>
    <div id="content-body">
       


<article id="post-CS50-AI" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img" rel="gallery_cmas3ri5000018fla63focjzs">
        <img src="/images/cs50ai.png" itemprop="image">
      </a>
    
  </div>
</div>

   
  <div class="article-inner">
    <div class="article-main">
      <header class="article-header">
        
<div class="main-title-bar">
  <div class="main-title-dot"></div>
  
    
      <h1 class="p-name article-title" itemprop="headline name">
        CS50-AI
      </h1>
    
  
</div>

        <div class='meta-info-bar'>
          <div class="meta-info">
  <time class="dt-published" datetime="2025-05-05T08:44:36.000Z" itemprop="datePublished">2025-05-05</time>
</div>
          <div class="need-seperator meta-info">
            <div class="meta-cate-flex">
  
  <a class="meta-cate-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a>
   
</div>
  
          </div>
          <div class="wordcount need-seperator meta-info">
            23k words 
          </div>
          <!-- busuanzi-counter -->
          
            
            <div class="readcount need-seperator meta-info">
             <span id="busuanzi_container_page_pv">阅读<span id="busuanzi_value_page_pv"></span>次</span>
            </div>
          
        </div>
        
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/csdiy/" rel="tag">csdiy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag">学习</a></li></ul>

      </header>
      <div class="e-content article-entry" itemprop="articleBody">
        
            <!-- Table of Contents -->
             
          <blockquote>
<p>This blog is used to note down all the knowledge I’ve learned in this lesson.</p>
</blockquote>
<p>Here is some links to this lesson.</p>
<p><a target="_blank" rel="noopener" href="https://cs50.harvard.edu/ai/">Home Page</a> and <a target="_blank" rel="noopener" href="https://cs50.me/cs50ai">Gradebook</a></p>
<p>About submitting projects:</p>
<p><a target="_blank" rel="noopener" href="https://cs50.readthedocs.io/projects/check50/en/latest/index.html">check50</a> , <a target="_blank" rel="noopener" href="https://cs50.readthedocs.io/submit50/">submit50</a> , <a target="_blank" rel="noopener" href="https://github.com/me50/smiling689">my submit</a> , <a target="_blank" rel="noopener" href="https://submit.cs50.io/courses/2072/">Pass_submit</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/smiling689/CS50-AI">Here</a> is my solution to labs and the course resources including slides and code ex.</p>
<h2 id="Lecture-0-Search"><a href="#Lecture-0-Search" class="headerlink" title="Lecture 0 Search"></a>Lecture 0 Search</h2><h3 id="uninformed-search"><a href="#uninformed-search" class="headerlink" title="uninformed search"></a>uninformed search</h3><p>DFS &amp;&amp; BFS</p>
<h3 id="informed-search"><a href="#informed-search" class="headerlink" title="informed search"></a>informed search</h3><h4 id="greedy-best-first-search"><a href="#greedy-best-first-search" class="headerlink" title="greedy best-first search"></a>greedy best-first search</h4><p>search algorithm that expands the node that is closest to the goal, as estimated by a heuristic function $h(n)$ </p>
<p>Heuristic function? Manhattan distance.</p>
<p><img src="/images/assets/image-20250503165654055.png" alt="image-20250503165654055" style="zoom:33%;" /></p>
<p>Choose the best way that minimize the heuristic function every time. It makes decision locally.</p>
<p>But this greedy algorithm won’t always find the best way.(The shortest)</p>
<p><img src="/images/assets/image-20250503171030338.png" alt="image-20250503171030338" style="zoom:33%;" /></p>
<h4 id="A-search"><a href="#A-search" class="headerlink" title="$A^*$ search"></a>$A^*$ search</h4><p>search algorithm that expands node with lowest value of $g(n)+ h(n)$ </p>
<p>$g(n)$ = cost to reach node </p>
<p>$h(n)$ = estimated cost to goal</p>
<p><img src="/images/assets/image-20250503171006554.png" alt="image-20250503171006554" style="zoom:33%;" /></p>
<p>A* search optimal if </p>
<ul>
<li><p>$h(n)$ is admissible (never overestimates the true cost), and </p>
</li>
<li><p>$h(n)$ is consistent (for every node <em>n</em> and successor <em>n’</em> with step cost <em>c</em>, <em>h(n) ≤ h(n’) + c</em>)</p>
</li>
</ul>
<h3 id="Adversarial-Search"><a href="#Adversarial-Search" class="headerlink" title="Adversarial Search"></a>Adversarial Search</h3><p>eg.Tic-Tac-Toe</p>
<h4 id="Minimax"><a href="#Minimax" class="headerlink" title="Minimax"></a>Minimax</h4><p><strong>Minimax</strong> represents winning conditions as (-1) for one side and (+1) for the other side. Further actions will be driven by these conditions, with the minimizing side trying to get the lowest score, and the maximizer trying to get the highest score.</p>
<p><img src="/images/assets/image-20250503174337589.png" alt="image-20250503174337589" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250503174424345.png" alt="image-20250503174424345" style="zoom: 50%;" /></p>
<h4 id="Alpha-Beta-Pruning"><a href="#Alpha-Beta-Pruning" class="headerlink" title="Alpha-Beta Pruning"></a>Alpha-Beta Pruning</h4><p>Alpha and Beta are two values that you need to update, which means the best thing you can do so far and the worst thing you can do so far.</p>
<p><img src="/images/assets/image-20250503175333707.png" alt="image-20250503175333707" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250503175345593.png" alt="image-20250503175345593" style="zoom:33%;" /></p>
<h4 id="Depth-Limited-Minimax"><a href="#Depth-Limited-Minimax" class="headerlink" title="Depth-Limited Minimax"></a>Depth-Limited Minimax</h4><p>need an evaluation function : function that estimates the expected utility of the game from a given state</p>
<h2 id="Lecture-1-Knowledge"><a href="#Lecture-1-Knowledge" class="headerlink" title="Lecture 1 Knowledge"></a>Lecture 1 Knowledge</h2><h3 id="Propositional-Logic"><a href="#Propositional-Logic" class="headerlink" title="Propositional Logic"></a>Propositional Logic</h3><p>We’ve learned most of them in Mathematical Logic.</p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="Model-checking"><a href="#Model-checking" class="headerlink" title="Model checking"></a>Model checking</h4><p>To determine if KB ⊨ <em>α</em>:</p>
<p>• Enumerate all possible models. </p>
<p>• If in every model where KB is true, <em>α</em> is true, then KB entails <em>α</em>.</p>
<p>• Otherwise, KB does not entail <em>α</em></p>
<h3 id="Knowledge-Engineering"><a href="#Knowledge-Engineering" class="headerlink" title="Knowledge Engineering"></a>Knowledge Engineering</h3><h3 id="Inference-Rules"><a href="#Inference-Rules" class="headerlink" title="Inference Rules"></a>Inference Rules</h3><h3 id="Conjunctive-Normal-Form"><a href="#Conjunctive-Normal-Form" class="headerlink" title="Conjunctive Normal Form"></a>Conjunctive Normal Form</h3><p>A <strong>Clause</strong> is a disjunction of literals (a propositional symbol or a negation of a propositional symbol, such as P, ¬P). A <strong>disjunction</strong> consists of propositions that are connected with an Or logical connective (P ∨ Q ∨ R). A <strong>conjunction</strong>, on the other hand, consists of propositions that are connected with an And logical connective (P ∧ Q ∧ R). Clauses allow us to convert any logical statement into a <strong>Conjunctive Normal Form</strong> (CNF), which is a conjunction of clauses, for example: (A ∨ B ∨ C) ∧ (D ∨ ¬E) ∧ (F ∨ G).</p>
<p><img src="/images/assets/image-20250503203608917.png" alt="image-20250503203608917" style="zoom:33%;" /></p>
<p>Resolving a literal and its negation, i.e. ¬P and P, gives the <strong>empty clause</strong> (). The empty clause is always false, and this makes sense because it is impossible that both P and ¬P are true. This fact is used by the resolution algorithm.</p>
<ul>
<li>To determine if KB ⊨ α:<ul>
<li>Check: is (KB ∧ ¬α) a contradiction?<ul>
<li>If so, then KB ⊨ α.</li>
<li>Otherwise, no entailment.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Proof by contradiction is a tool used often in computer science. If our knowledge base is true, and it contradicts ¬α, it means that ¬α is false, and, therefore, α must be true. More technically, the algorithm would perform the following actions:</p>
<ul>
<li>To determine if KB ⊨ α:<ul>
<li>Convert (KB ∧ ¬α) to Conjunctive Normal Form.</li>
<li>Keep checking to see if we can use resolution to produce a new clause.</li>
<li>If we ever produce the empty clause (equivalent to False), congratulations! We have arrived at a contradiction, thus proving that KB ⊨ α.</li>
<li>However, if contradiction is not achieved and no more clauses can be inferred, there is no entailment.</li>
</ul>
</li>
</ul>
<h3 id="First-Order-Logic"><a href="#First-Order-Logic" class="headerlink" title="First-Order Logic"></a>First-Order Logic</h3><h2 id="Lecture-2-Uncertainty"><a href="#Lecture-2-Uncertainty" class="headerlink" title="Lecture 2 Uncertainty"></a>Lecture 2 Uncertainty</h2><h3 id="Conditional-probability"><a href="#Conditional-probability" class="headerlink" title="Conditional probability"></a>Conditional probability</h3><p>P(<em>a | b</em>), meaning “the probability of event <em>a</em> occurring given that we know event <em>b</em> to have occurred,” </p>
<p><img src="/images/assets/image-20250504150329209.png" alt="image-20250504150329209" style="zoom:33%;" /></p>
<h4 id="Random-Variables"><a href="#Random-Variables" class="headerlink" title="Random Variables"></a>Random Variables</h4><h4 id="Bayes’-Rule"><a href="#Bayes’-Rule" class="headerlink" title="Bayes’ Rule"></a>Bayes’ Rule</h4><p><img src="/images/assets/image-20250504154011759.png" alt="image-20250504154011759" style="zoom:33%;" /></p>
<h4 id="Conditioning"><a href="#Conditioning" class="headerlink" title="Conditioning"></a>Conditioning</h4><p><img src="/images/assets/image-20250504154724873.png" alt="image-20250504154724873" style="zoom:33%;" /></p>
<h3 id="Bayesian-Networks"><a href="#Bayesian-Networks" class="headerlink" title="Bayesian Networks"></a>Bayesian Networks</h3><p><img src="/images/assets/image-20250504155732710.png" alt="image-20250504155732710" style="zoom:50%;" /></p>
<h4 id="Inferences"><a href="#Inferences" class="headerlink" title="Inferences"></a>Inferences</h4><h5 id="Inference-by-Enumeration"><a href="#Inference-by-Enumeration" class="headerlink" title="Inference by Enumeration"></a><strong>Inference by Enumeration</strong></h5><p><img src="/images/assets/image-20250504160852268.png" alt="image-20250504160852268" style="zoom:15%;" /></p>
<h4 id="Approximate-Inference"><a href="#Approximate-Inference" class="headerlink" title="Approximate Inference"></a>Approximate Inference</h4><h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a>Sampling</h3><h4 id="Likelihood-Weighting"><a href="#Likelihood-Weighting" class="headerlink" title="Likelihood Weighting"></a>Likelihood Weighting</h4><ul>
<li>Sample the non-evidence variables using conditional probabilities in the Bayesian network.</li>
<li>Weight each sample by its <strong>likelihood</strong>: the probability of all the evidence occurring.</li>
</ul>
<h3 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h3><p>The probability changed due to time changes</p>
<h4 id="The-Markov-Assumption"><a href="#The-Markov-Assumption" class="headerlink" title="The Markov Assumption"></a>The Markov Assumption</h4><p>the assumption that the current state depends on only a finite fixed number of previous states</p>
<h4 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h4><p>a sequence of random variables where the distribution of each variable follows the Markov assumption</p>
<p>To start constructing a Markov chain, we need a <strong>transition model</strong> that will specify the the probability distributions of the next event based on the possible values of the current event.</p>
<h3 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h3><p>a Markov model for a system with hidden states that generate some observed event</p>
<p>Our AI wants to infer the weather (the hidden state), but it only has access to an indoor camera that records how many people brought umbrellas with them. Here is our <strong>sensor model</strong> (also called <strong>emission model</strong>) that represents these probabilities:</p>
<p><img src="/images/assets/image-20250504164044394.png" alt="image-20250504164044394" style="zoom:33%;" /></p>
<h4 id="sensor-Markov-assumption"><a href="#sensor-Markov-assumption" class="headerlink" title="sensor Markov assumption"></a>sensor Markov assumption</h4><p>the assumption that the evidence variable depends only the corresponding state</p>
<h2 id="Lecture-3-Optimization"><a href="#Lecture-3-Optimization" class="headerlink" title="Lecture 3 Optimization"></a>Lecture 3 Optimization</h2><p><img src="/images/assets/image-20250505151248761.png" alt="image-20250505151248761" style="zoom:50%;" /></p>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>choosing the best option from a set of options</p>
<h3 id="local-search"><a href="#local-search" class="headerlink" title="local search"></a>local search</h3><p>search algorithms that maintain a single node and searches by moving to a neighboring node</p>
<p>Construct a state-space landscape. We’re going to find the global maximum or global minimum.</p>
<p><img src="/images/assets/image-20250505101104121.png" alt="image-20250505101104121" style="zoom:33%;" /></p>
<h3 id="Hill-Climbing"><a href="#Hill-Climbing" class="headerlink" title="Hill Climbing"></a>Hill Climbing</h3><p>function Hill-Climb(<em>problem</em>):</p>
<ul>
<li><em>current</em> = initial state of <em>problem</em></li>
<li>repeat:<ul>
<li><em>neighbor</em> = best valued neighbor of <em>current</em></li>
<li>if <em>neighbor</em> not better than <em>current</em> :<ul>
<li>return <em>current</em></li>
</ul>
</li>
<li><em>current</em> = <em>neighbor</em></li>
</ul>
</li>
</ul>
<h4 id="Local-and-Global-Minima-and-Maxima"><a href="#Local-and-Global-Minima-and-Maxima" class="headerlink" title="Local and Global Minima and Maxima"></a><strong>Local and Global Minima and Maxima</strong></h4><p>shoulders, where multiple states of equal value are adjacent and the neighbors of the plateau can be both better and worse</p>
<h3 id="Hill-Climbing-Variants"><a href="#Hill-Climbing-Variants" class="headerlink" title="Hill Climbing Variants"></a><strong>Hill Climbing Variants</strong></h3><ul>
<li><strong>Steepest-ascent</strong>: choose the highest-valued neighbor. This is the standard variation that we discussed above.</li>
<li><strong>Stochastic</strong>: choose randomly from higher-valued neighbors. Doing this, we choose to go to any direction that improves over our value. This makes sense if, for example, the highest-valued neighbor leads to a local maximum while another neighbor leads to a global maximum.</li>
<li><strong>First-choice</strong>: choose the first higher-valued neighbor.</li>
<li><strong>Random-restart</strong>: conduct hill climbing multiple times. Each time, start from a random state. Compare the maxima from every trial, and choose the highest amongst those.</li>
<li><strong>Local Beam Search</strong>: chooses the <em>k</em> highest-valued neighbors. This is unlike most local search algorithms in that it uses multiple nodes for the search, and not just one.</li>
</ul>
<h3 id="Simulated-Annealing"><a href="#Simulated-Annealing" class="headerlink" title="Simulated Annealing"></a>Simulated Annealing</h3><p>function Simulated-Annealing(<em>problem</em>, <em>max</em>):</p>
<ul>
<li><em>current</em> = initial state of <em>problem</em></li>
<li>for t = 1 to max:<ul>
<li><em>T</em> = Temperature(<em>t</em>)</li>
<li><em>neighbor</em> = random neighbor of <em>current</em></li>
<li><em>ΔE</em> = how much better <em>neighbor</em> is than <em>current</em></li>
<li>if ΔE &gt; 0:<ul>
<li><em>current</em> = <em>neighbor</em></li>
</ul>
</li>
<li>with probability e^(<em>ΔE/T</em>) set <em>current</em> = <em>neighbor</em></li>
</ul>
</li>
<li>return <em>current</em></li>
</ul>
<p>The algorithm takes as input a problem and <em>max</em>, the number of times it should repeat itself. For each iteration, <em>T</em> is set using a Temperature function. This function returns a higher value in the early iterations (when <em>t</em> is low) and a lower value in later iterations (when <em>t</em> is high). </p>
<h4 id="Traveling-Salesman-Problem"><a href="#Traveling-Salesman-Problem" class="headerlink" title="Traveling Salesman Problem"></a><strong>Traveling Salesman Problem</strong></h4><h3 id="Linear-Programming"><a href="#Linear-Programming" class="headerlink" title="Linear Programming"></a>Linear Programming</h3><ul>
<li>Simplex </li>
<li>Interior-Point</li>
</ul>
<h3 id="Constraint-Satisfaction"><a href="#Constraint-Satisfaction" class="headerlink" title="Constraint Satisfaction"></a>Constraint Satisfaction</h3><p>Constraint Satisfaction problems are a class of problems where variables need to be assigned values while satisfying some conditions.</p>
<p>Constraints satisfaction problems have the following properties:</p>
<ul>
<li>Set of variables (x₁, x₂, …, xₙ)</li>
<li>Set of domains for each variable {D₁, D₂, …, Dₙ}</li>
<li>Set of constraints C</li>
</ul>
<p><img src="/images/assets/image-20250505112452972.png" alt="image-20250505112452972" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250505112500696.png" alt="image-20250505112500696" style="zoom:33%;" /></p>
<p>A few more terms worth knowing about constraint satisfaction problems:</p>
<ul>
<li>A <strong>Hard Constraint</strong> is a constraint that must be satisfied in a correct solution.</li>
<li>A <strong>Soft Constraint</strong> is a constraint that expresses which solution is preferred over others.</li>
<li>A <strong>Unary Constraint</strong> is a constraint that involves only one variable. In our example, a unary constraint would be saying that course A can’t have an exam on Monday {<em>A ≠ Monday</em>}.</li>
<li>A <strong>Binary Constraint</strong> is a constraint that involves two variables. This is the type of constraint that we used in the example above, saying that some two courses can’t have the same value {<em>A ≠ B</em>}.</li>
</ul>
<h4 id="Node-Consistency"><a href="#Node-Consistency" class="headerlink" title="Node Consistency"></a>Node Consistency</h4><p>Node consistency is when all the values in a variable’s domain satisfy the variable’s unary constraints.</p>
<p>Else we’ll remove something from the domain in order to satisfy the constraint.</p>
<h4 id="Arc-Consistency"><a href="#Arc-Consistency" class="headerlink" title="Arc Consistency"></a>Arc Consistency</h4><p>Arc consistency is when all the values in a variable’s domain satisfy the variable’s binary constraints (note that we are now using “arc” to refer to what we previously referred to as “edge”). In other words, to make X arc-consistent with respect to Y, remove elements from X’s domain until every choice for X has a possible choice for Y.</p>
<p>When X choose any one in X’s domain, there is at least one choice that Y can choose in Y’s domain that satisfy Y’s constraint. If not, we need to remove something from X’s domain.</p>
<p>function Revise(<em>csp, X, Y</em>):</p>
<ul>
<li><em>revised</em> = <em>false</em></li>
<li>for <em>x</em> in <em>X.domain</em>:<ul>
<li>if no <em>y</em> in <em>Y.domain</em> satisfies constraint for <em>(X,Y)</em>:<ul>
<li>delete <em>x</em> from <em>X.domain</em></li>
<li><em>revised</em> = <em>true</em></li>
</ul>
</li>
</ul>
</li>
<li>return <em>revised</em></li>
</ul>
<p>Often we are interested in making the whole problem arc-consistent and not just one variable with respect to another. In this case, we will use an algorithm called AC-3, which uses Revise:</p>
<p>function AC-3(<em>csp</em>):</p>
<ul>
<li><em>queue</em> = all arcs in <em>csp</em></li>
<li>while <em>queue</em> non-empty:<ul>
<li>(<em>X, Y</em>) = Dequeue(<em>queue</em>)</li>
<li>if Revise(csp, X, Y):<ul>
<li>if size of <em>X.domain</em> == 0:<ul>
<li>return <em>false</em></li>
</ul>
</li>
<li>for each <em>Z</em> in <em>X.neighbors - {Y}</em>:<ul>
<li>Enqueue(queue, (<em>Z,X</em>))</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>return <em>true</em></li>
</ul>
<p>While the algorithm for arc consistency can simplify the problem, it will not necessarily solve it, since it considers binary constraints only and not how multiple nodes might be interconnected. </p>
<p>CSPs as Search Problems</p>
<ul>
<li>initial state: empty assignment (no variables) </li>
<li>actions: add a {<em>variable</em> = <em>value</em>} to assignment </li>
<li>transition model: shows how adding an assignment changes the assignment </li>
<li>goal test: check if all variables assigned and constraints all satisfied </li>
<li>path cost function: all paths have same cost</li>
</ul>
<h3 id="Backtracking-Search"><a href="#Backtracking-Search" class="headerlink" title="Backtracking Search"></a>Backtracking Search</h3><p>function Backtrack(<em>assignment, csp</em>):</p>
<ul>
<li>if <em>assignment</em> complete:<ul>
<li>return <em>assignment</em></li>
</ul>
</li>
<li><em>var</em> = Select-Unassigned-Var(<em>assignment, csp</em>)</li>
<li>for <em>value</em> in Domain-Values(var, assignment, csp):<ul>
<li>if <em>value</em> consistent with <em>assignment</em>:<ul>
<li>add {<em>var = value</em>} to <em>assignment</em></li>
<li><em>result</em> = Backtrack(<em>assignment, csp</em>)</li>
<li>if <em>result</em> ≠ <em>failure</em>:<ul>
<li>return <em>result</em></li>
</ul>
</li>
<li><em>remove</em> {<em>var = value</em>} from <em>assignment</em></li>
</ul>
</li>
</ul>
</li>
<li>return <em>failure</em></li>
</ul>
<h3 id="Inference-1"><a href="#Inference-1" class="headerlink" title="Inference"></a>Inference</h3><p><strong>maintaining arc-consistency</strong></p>
<p>algorithm for enforcing arc-consistency every time we make a new assignment</p>
<p>When we make a new assignment to <em>X</em>, calls AC-3, starting with a queue of all arcs (<em>Y</em>, <em>X</em>)where <em>Y</em> is a neighbor of <em>X</em></p>
<p>function Backtrack(<em>assignment, csp</em>):</p>
<ul>
<li>if <em>assignment</em> complete:<ul>
<li>return <em>assignment</em></li>
</ul>
</li>
<li><em>var</em> = <strong>Select-Unassigned-Var</strong>(<em>assignment, csp</em>)</li>
<li>for <em>value</em> in <strong>Domain-Values</strong>(<em>var, assignment, csp</em>):<ul>
<li>if <em>value</em> consistent with <em>assignment</em>:<ul>
<li><strong>add {<em>var</em> = <em>value</em>} to <em>assignment</em></strong></li>
<li><strong><em>inferences</em> = Inference(<em>assignment, csp</em>)</strong></li>
<li>if <em>inferences</em> ≠ <em>failure</em>:<ul>
<li>add <em>inferences</em> to <em>assignment</em></li>
</ul>
</li>
<li><em>result</em> = Backtrack(<em>assignment, csp</em>)</li>
<li>if <em>result</em> ≠ <em>failure</em>:<ul>
<li>return <em>result</em></li>
</ul>
</li>
<li><em>remove</em> {<em>var = value</em>} <strong>and <em>inferences</em></strong> from <em>assignment</em></li>
</ul>
</li>
</ul>
</li>
<li>return <em>failure</em></li>
</ul>
<p>SELECT-UNASSIGNED-VAR</p>
<ul>
<li>minimum remaining values (MRV)** heuristic: select the variable that has the smallest domain </li>
<li><strong>degree</strong> heuristic: select the variable that has the highest degree</li>
</ul>
<p>DOMAIN-VALUES</p>
<ul>
<li><strong>least-constraining values</strong> heuristic: return variables in order by number of choices that are ruled out for neighboring variables <ul>
<li>try least-constraining values first</li>
</ul>
</li>
</ul>
<h2 id="Lecture-4-Learning"><a href="#Lecture-4-Learning" class="headerlink" title="Lecture 4 Learning"></a>Lecture 4 Learning</h2><h3 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h3><p>Three categories : Supervised Learning, Reinforcement Learning and Unsupervised Learning.</p>
<h3 id="1-Supervised-Learning"><a href="#1-Supervised-Learning" class="headerlink" title="1) Supervised Learning"></a>1) Supervised Learning</h3><p>given a data set of input-output pairs, learn a function to map inputs to outputs</p>
<h3 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h3><p>supervised learning task of learning a function mapping an input point to a discrete category</p>
<p>There are lots of ways to classify:</p>
<h4 id="1-k-nearest-neighbor-classification"><a href="#1-k-nearest-neighbor-classification" class="headerlink" title="1. k-nearest-neighbor classification"></a>1. <em>k</em>-nearest-neighbor classification</h4><p>algorithm that, given an input, chooses the most common class out of the <em>k</em> nearest data points to that input</p>
<p><img src="/images/assets/image-20250505164312683.png" alt="image-20250505164312683" style="zoom:33%;" /></p>
<h4 id="2-Perceptron-Learning"><a href="#2-Perceptron-Learning" class="headerlink" title="2. Perceptron Learning"></a>2. Perceptron Learning</h4><p>Another way of going about a classification problem, as opposed to the nearest-neighbor strategy, is looking at the data as a whole and trying to create a decision boundary.</p>
<p><img src="/images/assets/image-20250505164411282.png" alt="image-20250505164411282" style="zoom:33%;" /></p>
<p>Goal:</p>
<p><img src="/images/assets/image-20250505164828002.png" alt="image-20250505164828002" style="zoom:33%;" /></p>
<p>Learning Rule:</p>
<p><img src="/images/assets/image-20250505165834360.png" alt="image-20250505165834360" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250505165906523.png" alt="image-20250505165906523" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250505170343145.png" alt="image-20250505170343145" style="zoom:25%;" /></p>
<p>The problem with this type of function is that it is unable to express uncertainty, since it can only be equal to 0 or to 1. It employs a <strong>hard threshold</strong>. A way to go around this is by using a logistic function, which employs a <strong>soft threshold</strong>. A logistic function can yield a real number between 0 and 1, which will express confidence in the estimate. The closer the value to 1, the more likely it is to rain.</p>
<p><img src="/images/assets/image-20250505170355935.png" alt="image-20250505170355935" style="zoom:25%;" /></p>
<h4 id="3-Support-Vector-Machines"><a href="#3-Support-Vector-Machines" class="headerlink" title="3. Support Vector Machines"></a>3. Support Vector Machines</h4><p>Trying to find </p>
<h5 id="maximum-margin-separator"><a href="#maximum-margin-separator" class="headerlink" title="maximum margin separator"></a>maximum margin separator</h5><p>boundary that maximizes the distance between any of the data points</p>
<p>It can be non-linear.</p>
<p><img src="/images/assets/image-20250505172212461.png" alt="image-20250505172212461" style="zoom:33%;" /></p>
<h3 id="Regression"><a href="#Regression" class="headerlink" title="Regression"></a>Regression</h3><p>supervised learning task of learning a function mapping an input point to a continuous value</p>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>function that expresses how poorly our hypothesis performs</p>
<p>For classification problems, we can use a <strong>0-1 Loss Function</strong>.</p>
<ul>
<li>L(actual, predicted):<ul>
<li>0 if actual = predicted</li>
<li>1 otherwise</li>
</ul>
</li>
</ul>
<p><img src="/images/assets/image-20250505172858727.png" alt="image-20250505172858727" style="zoom:33%;" /></p>
<p>L₁ and L₂ loss functions can be used when predicting a continuous value. In this case, we are interested in quantifying for each prediction <em>how much</em> it differed from the observed value. We do this by taking either the absolute value or the squared value of the observed value minus the predicted value (i.e. how far the prediction was from the observed value).</p>
<ul>
<li>L₁: <em>L</em>(actual, predicted) = |actual - predicted|</li>
<li>L₂: <em>L</em>(actual, predicted) = (actual - predicted)²</li>
</ul>
<p><img src="/images/assets/image-20250505173016404.png" alt="image-20250505173016404" style="zoom:33%;" /></p>
<h3 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h3><p>a model that fits too closely to a particular data set and therefore may fail to generalize to future data</p>
<p><img src="/images/assets/image-20250505173313827.png" alt="image-20250505173313827" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250505173323879.png" alt="image-20250505173323879" style="zoom:33%;" /></p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><p>penalizing hypotheses that are more complex to favor simpler, more general hypotheses </p>
<p><em>cost</em>(h) = <em>loss</em>(h) + λ<em>complexity</em>(h)</p>
<p>If the model is too complex, like the two graphs up there, we should give some penalty to avoid overfitting.</p>
<h4 id="holdout-cross-validation"><a href="#holdout-cross-validation" class="headerlink" title="holdout cross-validation"></a>holdout cross-validation</h4><p>splitting data into a <strong>training set</strong> and a <strong>test set</strong>, such that learning happens on the training set and is evaluated on the test set</p>
<h4 id="k-fold-cross-validation"><a href="#k-fold-cross-validation" class="headerlink" title="k-fold cross-validation"></a><em>k</em>-fold cross-validation</h4><p>splitting data into <em>k</em> sets, and experimenting <em>k</em> times, using each set as a test set once, and using remaining data as training set</p>
<h3 id="scikit-learn"><a href="#scikit-learn" class="headerlink" title="scikit-learn"></a>scikit-learn</h3><p>As often is the case with Python, there are multiple libraries that allow us to conveniently use machine learning algorithms. One of such libraries is scikit-learn.</p>
<h3 id="2-Reinforcement-Learning"><a href="#2-Reinforcement-Learning" class="headerlink" title="2) Reinforcement Learning"></a>2) Reinforcement Learning</h3><p>given a set of rewards or punishments, learn what actions to take in the future</p>
<p><img src="/images/assets/image-20250505182845191.png" alt="image-20250505182845191" style="zoom:33%;" /></p>
<h4 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h4><p>model for decision-making, representing states, actions, and their rewards</p>
<p>Reinforcement learning can be viewed as a Markov decision process, having the following properties:</p>
<ul>
<li>Set of states <strong><em>S</em></strong></li>
<li>Set of actions <strong><em>Actions(S)</em></strong></li>
<li>Transition model <strong><em>P(s’ | s, a)</em></strong></li>
<li>Reward function <strong><em>R(s, a, s’)</em></strong></li>
</ul>
<h4 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h4><p>one model of reinforcement learning, where a function <strong><em>Q(s, a)</em></strong> outputs an estimate of the value of taking action <em>a</em> in state <em>s</em>.</p>
<p>The model starts with all estimated values equal to 0 (<strong><em>Q(s,a)</em> = 0</strong> for all <em>s, a</em>). When an action is taken and a reward is received, the function does two things: 1) it estimates the value of <strong><em>Q(s, a)</em></strong> based on current reward and expected future rewards, and 2) updates <strong><em>Q(s, a)</em></strong> to take into account both the old estimate and the new estimate. This gives us an algorithm that is capable of improving upon its past knowledge without starting from scratch.</p>
<p><img src="/images/assets/image-20250505183952324.png" alt="image-20250505183952324" style="zoom:33%;" /></p>
<p>$\alpha$ : learning rate, represent how much we value new information compared to old information. </p>
<p>r is the reward of this step. What’s more, we can add some future rewards into it:</p>
<p><img src="/images/assets/image-20250505184252428.png" alt="image-20250505184252428" style="zoom:33%;" /></p>
<p>For example:</p>
<p><img src="/images/assets/image-20250505184304661.png" alt="image-20250505184304661" style="zoom:33%;" /></p>
<p>Or:</p>
<p><img src="/images/assets/image-20250505184314960.png" alt="image-20250505184314960" style="zoom:33%;" /></p>
<h5 id="Greedy-Decision-Making"><a href="#Greedy-Decision-Making" class="headerlink" title="Greedy Decision-Making"></a>Greedy Decision-Making</h5><p>When in state <em>s</em>, choose action <em>a</em> with highest <em>Q</em>(<em>s</em>, <em>a</em>)</p>
<p>This brings us to discuss the <strong>Explore vs. Exploit</strong> tradeoff. A greedy algorithm always exploits, taking the actions that are already established to bring to good outcomes. However, it will always follow the same path to the solution, never finding a better path.</p>
<h5 id="ε-greedy"><a href="#ε-greedy" class="headerlink" title="ε-greedy"></a>ε-greedy</h5><ul>
<li>Set ε equal to how often we want to move randomly. </li>
<li>With probability 1 - ε, choose estimated best move. </li>
<li>With probability ε, choose a random move</li>
</ul>
<h4 id="function-approximation"><a href="#function-approximation" class="headerlink" title="function approximation"></a>function approximation</h4><p>approximating <em>Q</em>(<em>s</em>, <em>a</em>), often by a function combining various features, rather than storing one value for every state-action pair </p>
<h3 id="3-unsupervised-learning"><a href="#3-unsupervised-learning" class="headerlink" title="3) unsupervised learning"></a>3) unsupervised learning</h3><p>given input data without any additional feedback, learn patterns</p>
<h4 id="clustering"><a href="#clustering" class="headerlink" title="clustering"></a>clustering</h4><p>organizing a set of objects into groups in such a way that similar objects tend to be in the same group</p>
<h4 id="k-means-clustering"><a href="#k-means-clustering" class="headerlink" title="k-means clustering"></a><em>k</em>-means clustering</h4><p>algorithm for clustering data based on repeatedly assigning points to clusters and updating those clusters’ centers</p>
<p><img src="/images/assets/image-20250505190220959.png" alt="image-20250505190220959" style="zoom:33%;" /></p>
<p>Set k centers. Divide all the data for these clusters. Move these centers lots of time.</p>
<h2 id="Lecture-5-Neural-Networks"><a href="#Lecture-5-Neural-Networks" class="headerlink" title="Lecture 5 Neural Networks"></a>Lecture 5 Neural Networks</h2><h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><ul>
<li>Neurons are connected to and receive electrical signals from other neurons. </li>
<li>Neurons process input signals and can be activated</li>
</ul>
<h4 id="Artificial-Neural-Networks"><a href="#Artificial-Neural-Networks" class="headerlink" title="Artificial Neural Networks"></a>Artificial Neural Networks</h4><ul>
<li>Model mathematical function from inputs to outputs based on the structure and parameters of the network. </li>
<li>Allows for learning the network’s parameters based on data.</li>
</ul>
<p>When implemented in AI, the parallel of each neuron is a <strong>unit</strong> that’s connected to other units. For example, like in the last lecture, the AI might map two inputs, x₁ and x₂, to whether it is going to rain today or not. Last lecture, we suggested the following form for this hypothesis function: <em>h(x₁, x₂)</em> = <em>w₀ + w₁x₁ + w₂x₂</em>, where <em>w₁</em> and <em>w₂</em> are weights that modify the inputs, and <em>w₀</em> is a constant, also called <strong>bias</strong>, modifying the value of the whole expression.</p>
<h3 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h3><p><img src="/images/assets/image-20250505205850413.png" alt="image-20250505205850413" style="zoom:25%;" /></p>
<p><img src="/images/assets/image-20250505205858500.png" alt="image-20250505205858500" style="zoom:25%;" /></p>
<p><img src="/images/assets/image-20250505205906147.png" alt="image-20250505205906147" style="zoom:25%;" /></p>
<h3 id="Neural-Network-Structure"><a href="#Neural-Network-Structure" class="headerlink" title="Neural Network Structure"></a>Neural Network Structure</h3><p><img src="/images/assets/image-20250505210448275.png" alt="image-20250505210448275" style="zoom:33%;" /></p>
<p>Or logical connective can be presented as:</p>
<p><img src="/images/assets/image-20250505210457723.png" alt="image-20250505210457723" style="zoom: 33%;" /></p>
<p>We can visualize this function as a neural network. <em>x₁</em> is one input unit, and <em>x₂</em> is another input unit. They are connected to the output unit by an edge with a weight of 1. The output unit then uses function <em>g(-1 + 1x₁ + 2x₂)</em> with a threshold of 0 to output either 0 or 1 (false or true).</p>
<p><img src="/images/assets/image-20250505210626745.png" alt="image-20250505210626745" style="zoom:33%;" /></p>
<p>A similar process can be repeated with the And function (where the bias will be (-2)). Moreover, inputs and outputs don’t have to be distinct. A similar process can be used to take humidity and air pressure as input, and produce the probability of rain as output. </p>
<p><img src="/images/assets/image-20250505210813610.png" alt="image-20250505210813610" style="zoom:33%;" /></p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>algorithm for minimizing loss when training neural network</p>
<ul>
<li>Start with a random choice of weights. This is our naive starting place, where we don’t know how much we should weight each input.</li>
<li>Repeat:<ul>
<li>Calculate the gradient based on <strong>all data points</strong> that will lead to decreasing loss. Ultimately, the gradient is a vector (a sequence of numbers).</li>
<li>Update weights according to the gradient.</li>
</ul>
</li>
</ul>
<p>Stochastic Gradient Descent: Calculate the gradient based on <strong>one data point</strong></p>
<p>Mini-Batch Gradient Descent: Calculate the gradient based on <strong>one small batch</strong></p>
<p>This can be done with any number of inputs and outputs, where each input is connected to each output, and where the outputs represent decisions that we can make. Note that in this kind of neural networks the outputs are not connected. These output has no relation, mens that we can construct neural network one by one.</p>
<p><img src="/images/assets/image-20250505211834229.png" alt="image-20250505211834229" style="zoom:33%;" /></p>
<p>So far, our neural networks relied on <strong>perceptron</strong> output units. These are units that are only capable of learning a linear decision boundary, using a straight line to separate data. That is, based on a linear equation, the perceptron could classify an input to be one type or another (e.g. left picture). However, often, data are not linearly separable (e.g. right picture). In this case, we turn to multilayer neural networks to model data non-linearly.</p>
<p><img src="/images/assets/image-20250505212016234.png" alt="image-20250505212016234" style="zoom:30%;" /></p>
<h3 id="Multilayer-Neural-Networks"><a href="#Multilayer-Neural-Networks" class="headerlink" title="Multilayer Neural Networks"></a>Multilayer Neural Networks</h3><p>artificial neural network with an input layer, an output layer, and at least one hidden layer</p>
<p><img src="/images/assets/image-20250505212507143.png" alt="image-20250505212507143" style="zoom:25%;" /></p>
<h3 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h3><p>algorithm for training neural networks with hidden layers</p>
<ul>
<li>Calculate error for output layer</li>
<li>For each layer, starting with output layer and moving inwards towards earliest hidden layer:<ul>
<li>Propagate error back one layer. In other words, the current layer that’s being considered sends the errors to the preceding layer.</li>
<li>Update weights.</li>
</ul>
</li>
</ul>
<p>This can be extended to any number of hidden layers, creating <strong>deep neural networks</strong>, which are neural networks that have more than one hidden layer.</p>
<h3 id="Overfitting-1"><a href="#Overfitting-1" class="headerlink" title="Overfitting"></a>Overfitting</h3><h4 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h4><p>temporarily removing units — selected at random — from a neural network to prevent over-reliance on certain units</p>
<p><img src="/images/assets/image-20250505214252309.png" alt="image-20250505214252309" style="zoom:30%;" /></p>
<p>Note that after the training is finished, the whole neural network will be used again.</p>
<h3 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h3><p>You can experiment with TensorFlow neural networks in this <a target="_blank" rel="noopener" href="http://playground.tensorflow.org/">web application</a></p>
<h3 id="computer-vision"><a href="#computer-vision" class="headerlink" title="computer vision"></a>computer vision</h3><p>computational methods for analyzing and understanding digital images</p>
<p><img src="/images/assets/image-20250505221058736.png" alt="image-20250505221058736" style="zoom:25%;" /></p>
<p>Images consist of pixels, and pixels are represented by three values that range from 0 to 255, one for red, one for green and one for blue. These values are often referred to with the acronym RGB. We can use this to create a neural network where each color value in each pixel is an input, where we have some hidden layers, and the output is some number of units that tell us what it is that was shown in the image. However, there are a few drawbacks to this approach. First, by breaking down the image into pixels and the values of their colors, we can’t use the structure of the image as an aid. That is, as humans, if we see a part of a face we know to expect to see the rest of the face, and this quickens computation. We want to be able to use a similar advantage in our neural networks. Second, the sheer number of inputs is very big, which means that we will have to calculate a lot of weights.</p>
<h3 id="Image-Convolution"><a href="#Image-Convolution" class="headerlink" title="Image Convolution"></a>Image Convolution</h3><p>applying a filter that adds each pixel value of an image to its neighbors, weighted according to a kernel matrix</p>
<p>The kernel is the blue matrix, and the image is the big matrix on the left. The resulting filtered image is the small matrix on the bottom right. To filter the image with the kernel, we start with the pixel with value 20 in the top-left of the image (coordinates 1,1). Then, we will multiply all the values around it by the corresponding value in the kernel and sum them up (10<em>0 + 20</em>(-1) + 30<em>0 + 10</em>(-1) + 20<em>5 + 30</em>(-1) + 20<em>0 + 30</em>(-1) + 40*0), producing the value 10. </p>
<p><img src="/images/assets/image-20250505221545838.png" alt="image-20250505221545838" style="zoom:33%;" /></p>
<p>Different kernels can achieve different tasks. For edge detection, the following kernel is often used:</p>
<p><img src="/images/assets/image-20250505221640256.png" alt="image-20250505221640256" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250505221735377.png" alt="image-20250505221735377" style="zoom:25%;" /></p>
<p><img src="/images/assets/image-20250505221743309.png" alt="image-20250505221743309" style="zoom:25%;" /></p>
<p>The idea here is that when the pixel is similar to all its neighbors, they should cancel each other, giving a value of 0. Therefore, the more similar the pixels, the darker the part of the image, and the more different they are the lighter it is. Applying this kernel to an image (left) results in an image with pronounced edges (right):</p>
<p><img src="/images/assets/image-20250505221713930.png" alt="image-20250505221713930" style="zoom:25%;" /></p>
<p>We can use the PIL library (stands for Python Imaging Library) that can do most of the hard work for us.</p>
<p>Still, processing the image in a neural network is computationally expensive due to the number of pixels that serve as input to the neural network. </p>
<h4 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h4><p>reducing the size of an input by sampling from regions in the input</p>
<h4 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max-pooling"></a>max-pooling</h4><p>pooling by choosing the maximum value in each region</p>
<p><img src="/images/assets/image-20250505232602612.png" alt="image-20250505232602612" style="zoom:25%;" /></p>
<h3 id="convolutional-neural-network"><a href="#convolutional-neural-network" class="headerlink" title="convolutional neural network"></a>convolutional neural network</h3><p>neural networks that use convolution, usually for analyzing images</p>
<p><img src="/images/assets/image-20250505232852891.png" alt="image-20250505232852891" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250505232906756.png" alt="image-20250505232906756" style="zoom:33%;" /></p>
<h3 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h3><h4 id="feed-forward-neural-network"><a href="#feed-forward-neural-network" class="headerlink" title="feed-forward neural network"></a>feed-forward neural network</h4><p>neural network that has connections only in one direction</p>
<p><strong>Feed-Forward Neural Networks</strong> are the type of neural networks that we have discussed so far, where input data is provided to the network, which eventually produces some output. A diagram of how feed-forward neural networks work can be seen below.</p>
<p><img src="/images/assets/image-20250505235040156.png" alt="image-20250505235040156" style="zoom:33%;" /></p>
<h4 id="recurrent-neural-network"><a href="#recurrent-neural-network" class="headerlink" title="recurrent neural network"></a>recurrent neural network</h4><p>neural network that generates output that feeds back into its own inputs</p>
<p><img src="/images/assets/image-20250505235237126.png" alt="image-20250505235237126" style="zoom:33%;" /></p>
<p>As opposed to that, <strong>Recurrent Neural Networks</strong> consist of a non-linear structure, where the network uses its own output as input. For example, Microsoft’s <a target="_blank" rel="noopener" href="https://www.captionbot.ai/">captionbot</a> is capable of describing the content of an image with words in a sentence. </p>
<p>Recurrent neural networks are helpful in cases where the network deals with sequences and not a single individual object. Above, the neural network needed to produce a sequence of words. However, the same principle can be applied to analyzing video files, which consist of a sequence of images, or in translation tasks, where a sequence of inputs (words in the source language) is processed to produce a sequence of outputs (words in the target language).</p>
<p>Video analysis:</p>
<p><img src="/images/assets/image-20250506000004296.png" alt="image-20250506000004296" style="zoom:33%;" /></p>
<p>Translation:</p>
<p><img src="/images/assets/image-20250506000017045.png" alt="image-20250506000017045" style="zoom:33%;" /></p>
<h2 id="Lecture-6-Language"><a href="#Lecture-6-Language" class="headerlink" title="Lecture 6 Language"></a>Lecture 6 Language</h2><h3 id="Language"><a href="#Language" class="headerlink" title="Language"></a>Language</h3><p><strong>Natural Language Processing</strong> spans all tasks where the AI gets human language as input. The following are a few examples of such tasks:</p>
<ul>
<li>automatic summarization, where the AI is given text as input and it produces a summary of the text as output.</li>
<li>information extraction, where the AI is given a corpus of text and the AI extracts data as output.</li>
<li>language identification, where the AI is given text and returns the language of the text as output.</li>
<li>machine translation, where the AI is given a text in the origin language and it outputs the translation in the target language.</li>
<li>named entity recognition, where the AI is given text and it extracts the names of the entities in the text (for example, names of companies).</li>
<li>speech recognition, where the AI is given speech and it produces the same words in text.</li>
<li>text classification, where the AI is given text and it needs to classify it as some type of text.</li>
<li>word sense disambiguation, where the AI needs to choose the right meaning of a word that has multiple meanings (e.g. bank means both a financial institution and the ground on the sides of a river).</li>
</ul>
<h3 id="Syntax-and-Semantics"><a href="#Syntax-and-Semantics" class="headerlink" title="Syntax and Semantics"></a>Syntax and Semantics</h3><h3 id="Context-Free-Grammar"><a href="#Context-Free-Grammar" class="headerlink" title="Context-Free Grammar"></a>Context-Free Grammar</h3><p><img src="/images/assets/image-20250506095616567.png" alt="image-20250506095616567" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506095626982.png" alt="image-20250506095626982" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506095651854.png" alt="image-20250506095651854" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506095710705.png" alt="image-20250506095710705" style="zoom:33%;" /></p>
<h3 id="nltk"><a href="#nltk" class="headerlink" title="nltk"></a>nltk</h3><h3 id="n-grams"><a href="#n-grams" class="headerlink" title="n-grams"></a>n-grams</h3><p>a contiguous sequence of <em>n</em> items from a sample of text</p>
<h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>the task of splitting a sequence of </p>
<p>characters into pieces (tokens)</p>
<h3 id="Markov-Models-1"><a href="#Markov-Models-1" class="headerlink" title="Markov Models"></a>Markov Models</h3><p>As discussed in previous lectures, Markov models consist of nodes, the value of each of which has a probability distribution based on a finite number of previous nodes. Markov models can be used to generate text. To do so, we train the model on a text, and then establish probabilities for every <em>n</em>-th token in an <em>n</em>-gram based on the <em>n</em> words preceding it. For example, using trigrams, after the Markov model has two words, it can choose a third one from a probability distribution based on the first two. Then, it can choose a fourth word from a probability distribution based on the second and third words.</p>
<h3 id="Bag-of-words-model"><a href="#Bag-of-words-model" class="headerlink" title="Bag-of-words model"></a>Bag-of-words model</h3><p>model that represents text as an unordered collection of words</p>
<p><img src="/images/assets/image-20250506100517781.png" alt="image-20250506100517781" style="zoom:33%;" /></p>
<h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><p><img src="/images/assets/image-20250506100617659.png" alt="image-20250506100617659" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506100625422.png" alt="image-20250506100625422" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506100633674.png" alt="image-20250506100633674" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506100650344.png" alt="image-20250506100650344" style="zoom:33%;" /></p>
<h4 id="additive-smoothing"><a href="#additive-smoothing" class="headerlink" title="additive smoothing"></a>additive smoothing</h4><p>adding a value α to each value in our distribution to smooth the data</p>
<h4 id="Laplace-smoothing"><a href="#Laplace-smoothing" class="headerlink" title="Laplace smoothing"></a>Laplace smoothing</h4><p>adding 1 to each value in our distribution: pretending we’ve seen each value one more time than we actually have</p>
<h3 id="Word-Representation"><a href="#Word-Representation" class="headerlink" title="Word Representation"></a>Word Representation</h3><h4 id="one-hot-representation"><a href="#one-hot-representation" class="headerlink" title="one-hot representation"></a>one-hot representation</h4><p>representation of meaning as a vector with a single 1, and with other values as 0</p>
<p><img src="/images/assets/image-20250506100813654.png" alt="image-20250506100813654" style="zoom:33%;" /></p>
<p>However, while this representation works in a world with four words, if we want to represent words from a dictionary, when we can have 50,000 words, we will end up with 50,000 vectors of length 50,000. This is incredibly inefficient. Another problem in this kind of representation is that we are unable to represent similarity between words like “wrote” and “authored.”</p>
<h4 id="distributed-representation"><a href="#distributed-representation" class="headerlink" title="distributed representation"></a>distributed representation</h4><p>representation of meaning distributed across multiple values</p>
<p><img src="/images/assets/image-20250506100857830.png" alt="image-20250506100857830" style="zoom:33%;" /></p>
<h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>word2vec is an algorithm for generating distributed representations of words. It does so by <strong>Skip-Gram Architecture</strong>, which is a neural network architecture for predicting context given a target word.</p>
<p><img src="/images/assets/image-20250506100958562.png" alt="image-20250506100958562" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506101008226.png" alt="image-20250506101008226" style="zoom:33%;" /></p>
<h3 id="Neural-Networks-1"><a href="#Neural-Networks-1" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p>We use <strong>Recurrent neural networks</strong> :</p>
<p><img src="/images/assets/image-20250506101125985.png" alt="image-20250506101125985" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506101146760.png" alt="image-20250506101146760" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506101215024.png" alt="image-20250506101215024" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506101237039.png" alt="image-20250506101237039" style="zoom:33%;" /></p>
<p><img src="/images/assets/image-20250506101407640.png" alt="image-20250506101407640" style="zoom:33%;" /></p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><strong>Attention</strong> refers to the neural network’s ability to decide what values are more important than others.</p>
<p><img src="/images/assets/image-20250506101446257.png" alt="image-20250506101446257" style="zoom:33%;" /></p>
<h3 id="Transformers"><a href="#Transformers" class="headerlink" title="Transformers"></a>Transformers</h3><p><strong>Transformers</strong> is a new type of training architecture whereby each input word is passed through a neural network simultaneously. An input word goes into the neural network and is captured as an encoded representation. Because all words are fed into the neural network at the same time, word order could easily be lost. Accordingly, <strong>position encoding</strong> is added to the inputs. The neural network, therefore, will use both the word and the position of the word in the encoded representation. Additionally, a <strong>self-attention</strong> step is added to help define the context of the word being inputted. </p>
<p>Encode:</p>
<p><img src="/images/assets/image-20250506101611134.png" alt="image-20250506101611134" style="zoom:43%;" /></p>
<p>Decode:</p>
<p><img src="/images/assets/image-20250506101708037.png" alt="image-20250506101708037" style="zoom:43%;" /></p>

        
      </div>

         
    </div>
    
     
  </div>
  
    
<nav id="article-nav">
  <a class="article-nav-btn left "
    
      href="/2025/05/06/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"
      title="环境配置"
     >
    <i class="fa-solid fa-angle-left"></i>
    <p class="title-text">
      
        环境配置
        
    </p>
  </a>
  <a class="article-nav-btn right "
    
      href="/2025/04/30/ESet-report/"
      title="ESet-report"
     >

    <p class="title-text">
      
        ESet-report
        
    </p>
    <i class="fa-solid fa-angle-right"></i>
  </a>
</nav>


  
</article>


  
  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
  <div id="comment-card" class="comment-card">
    <div class="main-title-bar">
      <div class="main-title-dot"></div>
      <div class="main-title">Comments </div>
    </div>
    <div id="vcomments"></div>
  </div>
  <script>
      new Valine({"enable":true,"appId":"SfQIE69iSsJmTNkcRv2nKXfR-gzGzoHsz","appKey":"Qui0C4Ke95otxc0zdgAa3UwV","placeholder":"Write something ... (You should go to https://cn.gravatar.com and sign up to have your own profile picture)","pageSize":10,"highlight":true,"avatar":"identicon","serverURLs":null,"el":"#vcomments"});
  </script>





    </div>
    <div id="footer-wrapper">
      <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<footer id="footer">
  
  <div id="footer-info" class="inner">
    
    &copy; 2025 Smiling<br>
    Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & Theme <a target="_blank" rel="noopener" href="https://github.com/saicaca/hexo-theme-vivia">Vivia</a>
      <span id="busuanzi_container_site_pv">
        🌏本站总访问量<span id="busuanzi_value_site_pv"></span>次 |
      </span>
      <span id="busuanzi_container_site_uv">
        🧑‍💻本站访客数<span id="busuanzi_value_site_uv"></span>人次
      </span>
  </div>
</footer>

    </div>
    <div class="back-to-top-wrapper">
    <button id="back-to-top-btn" class="back-to-top-btn hide" onclick="topFunction()">
        <i class="fa-solid fa-angle-up"></i>
    </button>
</div>

<script>
    function topFunction() {
        window.scroll({ top: 0, behavior: 'smooth' });
    }
    let btn = document.getElementById('back-to-top-btn');
    function scrollFunction() {
        if (document.body.scrollTop > 600 || document.documentElement.scrollTop > 600) {
            btn.classList.remove('hide')
        } else {
            btn.classList.add('hide')
        }
    }
    window.onscroll = function() {
        scrollFunction();
    }
</script>

  </div>
  <script src="/js/light-dark-switch.js"></script>
</body>
</html>
